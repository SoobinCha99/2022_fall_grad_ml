{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbGB71XB1MVD"
      },
      "source": [
        "# 1. 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q_7_8lj0PWA"
      },
      "outputs": [],
      "source": [
        "'''!pip install torch\n",
        "!pip install transformers\n",
        "!pip install numpy'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p1o9Rzdp0PWC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\CHA SOOBIN\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from transformers import AutoModel, AutoTokenizer, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMX6ui-o1l3b"
      },
      "source": [
        "# 2. 데이터 로드\n",
        "\n",
        "Task 1. 과제 Task 1에서 만든 파일을 본인의 구글 드라이브에 올린 후 파일의 URL을 Data URLs에 각각 맞춰 업데이트 해주세요.\n",
        "기존에 적혀진 URL은 테스트를 위해 적은 양의 데이터만을 가진 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Task 1\\n# Data URLs\\ntraining_data_url = \"https://drive.google.com/file/d/1qCQg3_4ZCAKy0duN51sKCNdcdvmYNnTo/view?usp=sharing\"\\nvalid_data_url = \"https://drive.google.com/file/d/1rrQkVPkP2XSrGAlljhWs9VKAlSizDc3O/view?usp=sharing\"\\ntest_data_url = \"https://drive.google.com/file/d/1z4GSfOABgyyX7Lp1oPQp2G33T3F6Majx/view?usp=sharing\"\\n\\n# Training Data\\ntraining_data_file_id = training_data_url.split(\"/\")[-2]\\n!gdown $training_data_file_id\\n\\n# Validation Data\\nvalid_data_file_id = valid_data_url.split(\"/\")[-2]\\n!gdown $valid_data_file_id\\n\\n# Test Data\\ntest_data_file_id = test_data_url.split(\"/\")[-2]\\n!gdown $test_data_file_id'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# Task 1\n",
        "# Data URLs\n",
        "training_data_url = \"https://drive.google.com/file/d/1qCQg3_4ZCAKy0duN51sKCNdcdvmYNnTo/view?usp=sharing\"\n",
        "valid_data_url = \"https://drive.google.com/file/d/1rrQkVPkP2XSrGAlljhWs9VKAlSizDc3O/view?usp=sharing\"\n",
        "test_data_url = \"https://drive.google.com/file/d/1z4GSfOABgyyX7Lp1oPQp2G33T3F6Majx/view?usp=sharing\"\n",
        "\n",
        "# Training Data\n",
        "training_data_file_id = training_data_url.split(\"/\")[-2]\n",
        "!gdown $training_data_file_id\n",
        "\n",
        "# Validation Data\n",
        "valid_data_file_id = valid_data_url.split(\"/\")[-2]\n",
        "!gdown $valid_data_file_id\n",
        "\n",
        "# Test Data\n",
        "test_data_file_id = test_data_url.split(\"/\")[-2]\n",
        "!gdown $test_data_file_id'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n0zdLsTn0PWE"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"./processed_data/train.csv\")\n",
        "valid = pd.read_csv(\"./processed_data/valid.csv\")\n",
        "test = pd.read_csv(\"./processed_data/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ud_WMQc60PWE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(36744, 2)\n",
            "(4083, 2)\n",
            "(5122, 2)\n"
          ]
        }
      ],
      "source": [
        "print(train.shape)\n",
        "print(valid.shape)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W-YIpuMAtZ5y"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>직장에 새로운 신입사원이 입사를 했는데 알려줄게 너무 많아.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>집 앞에 슈퍼를 갈 때도 나를 자꾸 데려가.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>취직한 줄 알았던 아들이 알고 보니 백수였어. 오늘 피시방에서 놀고 있는 걸 발견했어.</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>내 악성 빈혈이 우리 손녀한테 유전되는 게 아닐까 걱정되네.</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>내 건강에 관련하여 고민이 되는 일이 있어.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentence  label\n",
              "0                 직장에 새로운 신입사원이 입사를 했는데 알려줄게 너무 많아.      0\n",
              "1                          집 앞에 슈퍼를 갈 때도 나를 자꾸 데려가.      0\n",
              "2  취직한 줄 알았던 아들이 알고 보니 백수였어. 오늘 피시방에서 놀고 있는 걸 발견했어.      3\n",
              "3                 내 악성 빈혈이 우리 손녀한테 유전되는 게 아닐까 걱정되네.      4\n",
              "4                          내 건강에 관련하여 고민이 되는 일이 있어.      2"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "r9Am41du0PWH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziU0btCD1xES"
      },
      "source": [
        "# 3. 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nFDMXiAr0PWG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 100%|██████████| 248k/248k [00:00<00:00, 285kB/s]  \n",
            "Downloading: 100%|██████████| 752k/752k [00:00<00:00, 758kB/s] \n",
            "Downloading: 100%|██████████| 173/173 [00:00<00:00, 173kB/s]\n"
          ]
        }
      ],
      "source": [
        "roberta = AutoModel.from_pretrained(\"klue/roberta-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJdp8j6BOUDa"
      },
      "source": [
        "Task 2. classifier에 맞는 내용을 코딩해주세요.\n",
        "\n",
        "Hint: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VH5A1QSA0PWH"
      },
      "outputs": [],
      "source": [
        "class RoBERTaClassifier(nn.Module):\n",
        "    def __init__(self, roberta, hidden_size=768, num_classes=2):\n",
        "        super(RoBERTaClassifier, self).__init__()\n",
        "        self.roberta = roberta\n",
        "        self.classifier = None # Task 2\n",
        "\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_masks)\n",
        "        return self.classifier(outputs.pooler_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GeKYJjiu0PWI"
      },
      "outputs": [],
      "source": [
        "model = RoBERTaClassifier(roberta=roberta).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHxAUVeT1tWa"
      },
      "source": [
        "# 4. 데이터셋 정의\n",
        "\n",
        "Task 3. ``document`` 변수에 적합한 코드를 작성하세요.\n",
        "\n",
        "Hint: ``self.labels`` 코드를 참고하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYvFfxt00PWG"
      },
      "outputs": [],
      "source": [
        "class KERDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset = dataset\n",
        "        document = None # Task 3\n",
        "        inputs = tokenizer(document, padding=True)\n",
        "        self.input_ids = inputs['input_ids']\n",
        "        self.attention_masks = inputs['attention_mask']\n",
        "        self.labels = self.dataset['label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.input_ids[idx], self.attention_masks[idx], self.labels[idx])       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD6n0SGk0PWG"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    input_ids = [item[0] for item in batch]\n",
        "    attention_masks = [item[1] for item in batch]\n",
        "    labels = [item[2] for item in batch]\n",
        "    return torch.LongTensor(input_ids), torch.LongTensor(attention_masks), torch.FloatTensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8y9g14D0PWH"
      },
      "outputs": [],
      "source": [
        "train_ds = KERDataset(train, tokenizer)\n",
        "valid_ds = KERDataset(valid, tokenizer)\n",
        "test_ds = KERDataset(test, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW2tqja_0PWH"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "log_interval = 400\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snpdWgOt0PWH"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, num_workers=5, collate_fn=collate_fn)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, num_workers=5, collate_fn=collate_fn)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, num_workers=5, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDMCLfaF0PWI"
      },
      "outputs": [],
      "source": [
        "total_steps = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(total_steps * warmup_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xgiq5_A0PWI"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_step,\n",
        "                                            num_training_steps = total_steps)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrQJhFIC2v6a"
      },
      "source": [
        "# 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zvF6LoD0PWI"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    correct = 0\n",
        "    X, Y = X.tolist(), Y.tolist()\n",
        "    for pred, label in zip(X, Y):\n",
        "        if pred.index(max(pred)) == label.index(max(label)):\n",
        "            correct += 1\n",
        "    train_acc = correct/len(X)\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enfpOsQJ0PWI"
      },
      "outputs": [],
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    valid_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (input_ids, attention_masks, labels) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_masks = attention_masks.to(device)\n",
        "        out = model(input_ids=input_ids, attention_masks=attention_masks)\n",
        "        labels = labels.tolist()\n",
        "        labels = torch.FloatTensor([[0, 1] if l == 0 else [1, 0] for l in labels]).to(device)\n",
        "        loss = loss_fn(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, labels)\n",
        "        \n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model.eval()\n",
        "    for batch_id, (input_ids, attention_masks, labels) in enumerate(tqdm_notebook(valid_dataloader)):\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_masks = attention_masks.to(device)\n",
        "        labels = torch.FloatTensor([[0, 1] if l == 0 else [1, 0] for l in labels]).to(device)\n",
        "        out = model(input_ids=input_ids, attention_masks=attention_masks)\n",
        "        valid_acc += calc_accuracy(out, labels)\n",
        "    print(\"epoch {} validation acc {}\".format(e+1, valid_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s8uLvZr22qz"
      },
      "source": [
        "# 6. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUItVok30PWJ"
      },
      "outputs": [],
      "source": [
        "test_acc = 0.0\n",
        "model.eval()\n",
        "for batch_id, (input_ids, attention_masks, labels) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_masks = attention_masks.to(device)\n",
        "    labels = torch.FloatTensor([[0, 1] if l == 0 else [1, 0] for l in labels]).to(device)\n",
        "    out = model(input_ids=input_ids, attention_masks=attention_masks)\n",
        "    test_acc += calc_accuracy(out, labels)\n",
        "print(\"Test acc : {}\".format(test_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbwHMhQhOqHh"
      },
      "source": [
        "Task 4. 기존 코드에서 문제점/아쉬운 점들에 대해서 논하세요. \n",
        "\n",
        "해당 내용은 여기 코드 및 결과와 함께 보고서로 작성해서 제출하세요.\n",
        "\n",
        "해당 문제점에 대한 해결 방안 및 이를 실제로 적용한 코드 작성 및 결과를 첨부한 경우 가산점이 있습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('nlp_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3d5f8d486bd050c94a6e9429297e6669528901ab188ae91bafb7a9ea614fd771"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
